#!/usr/bin/env python3
"""
Alice Agent - Jarvis-klon med lokal Ollama ist√§llet f√∂r Google Realtime
Bygger p√• original Friday Jarvis arkitektur
"""

import asyncio
import logging
from dotenv import load_dotenv

from livekit import agents
from livekit.agents import Agent, AgentSession, WorkerOptions, cli, RoomInputOptions
from livekit.plugins import silero
from livekit.agents.llm import LLM, LLMStream, ChatContext, ChatMessage
from livekit import rtc

from tools import AliceTools
import ollama

load_dotenv()

# Konfigurera logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("alice-agent")

AGENT_INSTRUCTION = """Du √§r Alice, en svensk AI-assistent som fungerar som en Jarvis-klon.

Du √§r hj√§lpsam, intelligent och kan:
- Svara p√• svenska (prim√§rt spr√•k)
- Hj√§lpa med v√§der, ber√§kningar, webbs√∂kning
- Ha naturliga konversationer
- Utf√∂ra praktiska uppgifter

H√•ll svaren naturliga och inte f√∂r l√•nga. Du pratar med anv√§ndaren via r√∂st.
Du √§r som en butler - artig, hj√§lpsam och professionell."""

class OllamaLLM(LLM):
    """Custom Ollama LLM wrapper f√∂r LiveKit Agents"""
    
    def __init__(self, model_name: str = "gpt-oss:20b"):
        self._model_name = model_name
        self._label = f"Ollama-{model_name}"
    
    @property 
    def label(self) -> str:
        return self._label
        
    async def chat(self, *, chat_ctx: ChatContext, conn_options=None, fnc_ctx=None) -> "LLMStream":
        """Implementera chat-funktionen f√∂r LiveKit"""
        # Konvertera LiveKit meddelanden till Ollama format
        messages = []
        for msg in chat_ctx.messages:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        try:
            response = ollama.chat(
                model=self._model_name,
                messages=messages,
                options={
                    "temperature": 0.8,
                    "top_p": 0.9,
                    "num_predict": 200
                }
            )
            
            # Skapa en enkel stream-simulation
            content = response['message']['content']
            logger.info(f"ü§ñ Ollama svarade: {content[:100]}...")
            
            # Returnera en mock LLMStream med inneh√•llet
            return MockLLMStream(content)
            
        except Exception as e:
            logger.error(f"‚ùå Ollama-fel: {e}")
            return MockLLMStream("Urs√§kta, jag har problem med AI-servern just nu.")

class MockLLMStream:
    """Enkel mock av LLMStream f√∂r Ollama"""
    
    def __init__(self, content: str):
        self.content = content
        self._done = False
    
    async def __aiter__(self):
        yield self.content
        self._done = True
    
    async def aclose(self):
        pass

class AliceAssistant(Agent):
    def __init__(self):
        # Alice anv√§nder verktyg fr√•n tools.py
        self.alice_tools = AliceTools()
        
        super().__init__(
            instructions=AGENT_INSTRUCTION,
            llm=OllamaLLM("gpt-oss:20b"),  # Lokal Ollama LLM
            # Kommenterar bort TTS f√∂r nu - fokus p√• att f√• anslutningen att fungera
            # tts=silero.TTS(language="sv", speaker="aidar"),  # Svenska TTS
            tools=[
                self.get_weather_tool,
                self.search_web_tool,
                self.calculate_tool,
                self.get_time_tool
            ]
        )
    
    async def get_weather_tool(self, city: str) -> str:
        """H√§mta v√§der f√∂r en stad"""
        logger.info(f"üå§Ô∏è H√§mtar v√§der f√∂r {city}")
        return self.alice_tools.get_weather(city)
    
    async def search_web_tool(self, query: str) -> str:
        """S√∂k p√• webben"""
        logger.info(f"üîç S√∂ker: {query}")
        return self.alice_tools.search_web(query, max_results=3)
    
    async def calculate_tool(self, expression: str) -> str:
        """Utf√∂r ber√§kning"""
        logger.info(f"üî¢ R√§knar: {expression}")
        return self.alice_tools.calculate(expression)
    
    async def get_time_tool(self) -> str:
        """H√§mta aktuell tid"""
        logger.info("‚è∞ H√§mtar tid")
        return self.alice_tools.get_time()

    async def generate_response(self, text: str) -> str:
        """Generera svar med lokal Ollama ist√§llet f√∂r Google Realtime"""
        try:
            logger.info(f"ü§ñ Alice genererar svar f√∂r: {text[:50]}...")
            
            response = ollama.chat(
                model="gpt-oss:20b",
                messages=[
                    {"role": "system", "content": AGENT_INSTRUCTION},
                    {"role": "user", "content": text}
                ],
                options={
                    "temperature": 0.8,  # Som original Jarvis
                    "top_p": 0.9,
                    "num_predict": 200  # Kortare f√∂r r√∂st
                }
            )
            
            answer = response['message']['content']
            logger.info(f"‚úÖ Alice svarade: {answer[:100]}...")
            return answer
            
        except Exception as e:
            logger.error(f"‚ùå Ollama-fel: {e}")
            return "Urs√§kta, jag har problem med AI-servern just nu."

async def entrypoint(ctx: agents.JobContext):
    """LiveKit Agent entrypoint - samma struktur som Jarvis"""
    logger.info("ü§ñ Alice startar...")
    
    # Skapa agent session (som original Jarvis)
    session = AgentSession()
    
    # Starta session med Alice agent
    await session.start(
        room=ctx.room,
        agent=AliceAssistant(),
        room_input_options=RoomInputOptions(
            video_enabled=True,
            audio_enabled=True,
            # Vi k√∂r lokalt s√• ingen noise cancellation fr√•n LiveKit Cloud
        )
    )
    
    logger.info("‚úÖ Alice session startad!")
    
    # Anslut till rummet
    await ctx.connect()
    logger.info("‚úÖ Alice ansluten till rummet!")

if __name__ == "__main__":
    # K√∂r agent med LiveKit CLI (exakt som Jarvis)
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))