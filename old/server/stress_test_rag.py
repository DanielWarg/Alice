#!/usr/bin/env python3
"""
Stresstest f√∂r Alice RAG/Context system
Testar prestanda under belastning och verifierar funktionalitet
"""

import asyncio
import time
import json
import random
import statistics
from concurrent.futures import ThreadPoolExecutor
from memory import MemoryStore
import os

# Testdata
SAMPLE_TEXTS = [
    "Alice √§r en avancerad AI-assistent som kan hantera komplexa uppgifter som e-post, musikuppspelning och planering.",
    "Gmail-integrationen g√∂r det m√∂jligt att skicka, l√§sa och s√∂ka e-post direkt genom naturligt spr√•k.",
    "RAG-systemet anv√§nder hybrid retrieval med BM25, recency scoring och kontextuell bonus f√∂r optimal relevance.",
    "Conversation context tracking sparar alla turns och f√∂rb√§ttrar minnesh√§mtning baserat p√• p√•g√•ende diskussion.",
    "Semantic chunking delar upp l√•nga texter i meningsfulla segment som respekterar paragraph och sentence boundaries.",
    "Local LLM integration med gpt-oss:20B via Ollama m√∂jligg√∂r privat och snabb AI-inferens utan externa API-anrop.",
    "Tool system architecture f√∂ljer single source of truth princip med centraliserade tool specs och executors.",
    "Pydantic models validerar alla tool arguments och s√§kerst√§ller type safety genom hela systemet.",
    "Svenska spr√•kst√∂d √§r inbyggt i alla komponenter fr√•n NLU till response generation och memory storage.",
    "Session-baserad context tracking grupperar konversationer per timme och modell f√∂r optimal minneshantering."
] * 10  # 100 texter

QUERIES = [
    "gmail e-post",
    "musik spela",
    "RAG retrieval",
    "context tracking", 
    "semantic chunking",
    "local LLM",
    "tool system",
    "svenska spr√•k",
    "AI assistent",
    "memory storage"
] * 5  # 50 queries

LONG_TEXTS = [
    """Detta √§r en mycket l√•ng text som testar semantic chunking funktionaliteten i Alice memory system. 

    F√∂rsta avsnittet handlar om hur RAG (Retrieval-Augmented Generation) fungerar med hybrid retrieval som kombinerar BM25 full-text search med recency scoring och kontextuell bonus. Detta ger mycket mer relevanta resultat √§n traditionella s√∂kmetoder.

    Andra avsnittet fokuserar p√• conversation context tracking som sparar alla user och assistant turns i sessioner baserade p√• modell och tid. Detta m√∂jligg√∂r att Alice kan f√∂rst√• vad som diskuterats tidigare i konversationen.

    Tredje avsnittet beskriver semantic chunking som intelligent delar upp l√•nga texter i mindre segment som respekterar paragraph och sentence boundaries ist√§llet f√∂r att bara dela p√• teckenantal.

    Fj√§rde och sista avsnittet t√§cker tekniska implementationsdetaljer som SQLite databas med WAL mode, FTS5 full-text indexering och optimerade SQL queries f√∂r snabb retrieval av relevanta memories.""",
    
    """Alice AI-assistenten har utvecklats med flera avancerade komponenter f√∂r att leverera en robust och intelligent anv√§ndarupplevelse.

    Gmail Integration bygger p√• Google API och till√•ter anv√§ndare att skicka e-post via naturligt spr√•k som "skicka mail till chef om projektuppdatering". Systemet hanterar OAuth autentisering s√§kert och sparar credentials lokalt.

    Tool Architecture f√∂ljer en centraliserad approach d√§r tool_specs.py √§r single source of truth f√∂r alla verktyg. Pydantic models validerar arguments och tool_registry.py hanterar execution med lambda functions.

    Memory System anv√§nder SQLite med flera tabeller: memories f√∂r textdata, conversations f√∂r context tracking, embeddings f√∂r semantic search och events f√∂r audit trail. FTS5 m√∂jligg√∂r snabb full-text search.

    Local LLM Support via Ollama k√∂r gpt-oss:20B modellen lokalt f√∂r privat inferens utan externa API-anrop. Detta s√§kerst√§ller b√•de prestanda och integritet f√∂r k√§nslig data."""
] * 5  # 10 l√•nga texter

class RAGStressTest:
    def __init__(self):
        self.db_path = "./stress_test_alice.db"
        self.memory = MemoryStore(self.db_path)
        self.results = {
            'insert_times': [],
            'retrieval_times': [],
            'context_times': [],
            'chunking_times': [],
            'errors': []
        }
    
    def cleanup(self):
        """Rensa testdatabasen"""
        if os.path.exists(self.db_path):
            os.remove(self.db_path)
    
    def setup_test_data(self):
        """F√∂rbereda testdata"""
        print("üîÑ Skapar testdata...")
        
        # L√§gg till normala texter
        start_time = time.time()
        for i, text in enumerate(SAMPLE_TEXTS):
            try:
                t0 = time.time()
                tags = json.dumps({"test": True, "batch": i // 10}, ensure_ascii=False)
                self.memory.upsert_text_memory_single(text, score=random.uniform(0, 5), tags_json=tags)
                self.results['insert_times'].append(time.time() - t0)
            except Exception as e:
                self.results['errors'].append(f"Insert error: {e}")
        
        # L√§gg till l√•nga texter (chunked)
        for i, long_text in enumerate(LONG_TEXTS):
            try:
                t0 = time.time()
                tags = json.dumps({"test": True, "long_text": True, "index": i}, ensure_ascii=False)
                memory_ids = self.memory.upsert_text_memory(long_text, score=3.0, tags_json=tags)
                chunk_time = time.time() - t0
                self.results['chunking_times'].append(chunk_time)
                print(f"üìù L√•ng text {i+1}: {len(memory_ids)} chunks p√• {chunk_time:.3f}s")
            except Exception as e:
                self.results['errors'].append(f"Chunking error: {e}")
        
        setup_time = time.time() - start_time
        print(f"‚úÖ {len(SAMPLE_TEXTS)} texter + {len(LONG_TEXTS)} l√•nga texter skapade p√• {setup_time:.2f}s")
    
    def test_retrieval_performance(self):
        """Test retrieval under belastning"""
        print("\nüîç Testar retrieval prestanda...")
        
        for query in QUERIES:
            try:
                # Test standard retrieval
                t0 = time.time()
                results = self.memory.retrieve_text_bm25_recency(query, limit=10)
                retrieval_time = time.time() - t0
                self.results['retrieval_times'].append(retrieval_time)
                
                if retrieval_time > 0.1:  # Warn if over 100ms
                    print(f"‚ö†Ô∏è  Slow query '{query}': {retrieval_time:.3f}s")
                    
            except Exception as e:
                self.results['errors'].append(f"Retrieval error for '{query}': {e}")
    
    def test_conversation_context(self):
        """Test conversation context under belastning"""
        print("\nüí¨ Testar conversation context...")
        
        # Skapa flera sessions
        sessions = [f"test_session_{i}" for i in range(10)]
        
        for session_id in sessions:
            try:
                # L√§gg till conversation turns
                t0 = time.time()
                for i in range(20):  # 20 turns per session
                    user_msg = f"User message {i} in {session_id}"
                    self.memory.add_conversation_turn(session_id, "user", user_msg)
                    
                    assistant_msg = f"Assistant response {i} in {session_id}"  
                    self.memory.add_conversation_turn(session_id, "assistant", assistant_msg)
                
                # Test context retrieval
                context = self.memory.get_conversation_context(session_id, limit=10)
                
                # Test context-aware memory retrieval
                for query in QUERIES[:3]:  # Test med n√•gra queries
                    memories = self.memory.get_related_memories_from_context(session_id, query, limit=5)
                
                context_time = time.time() - t0
                self.results['context_times'].append(context_time)
                
                print(f"üìä Session {session_id}: {len(context)} turns, {context_time:.3f}s")
                
            except Exception as e:
                self.results['errors'].append(f"Context error for {session_id}: {e}")
    
    def concurrent_stress_test(self):
        """Concurrent access test"""
        print("\n‚ö° Testar concurrent access...")
        
        def worker_task(worker_id):
            results = {'times': [], 'errors': []}
            try:
                for i in range(10):
                    t0 = time.time()
                    
                    # Random operation
                    operation = random.choice(['insert', 'retrieve', 'context'])
                    
                    if operation == 'insert':
                        text = f"Concurrent worker {worker_id} message {i}"
                        self.memory.upsert_text_memory_single(text, score=random.uniform(0,3))
                        
                    elif operation == 'retrieve':
                        query = random.choice(QUERIES)
                        memories = self.memory.retrieve_text_bm25_recency(query, limit=5)
                        
                    elif operation == 'context':
                        session = f"concurrent_session_{worker_id}"
                        self.memory.add_conversation_turn(session, "user", f"Message {i}")
                        context = self.memory.get_conversation_context(session, limit=5)
                    
                    results['times'].append(time.time() - t0)
                    
            except Exception as e:
                results['errors'].append(f"Worker {worker_id}: {e}")
            
            return results
        
        # K√∂r concurrent workers
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(worker_task, i) for i in range(5)]
            worker_results = [f.result() for f in futures]
        
        total_time = time.time() - start_time
        
        # Samla resultat
        all_times = []
        all_errors = []
        for result in worker_results:
            all_times.extend(result['times'])
            all_errors.extend(result['errors'])
        
        self.results['errors'].extend(all_errors)
        
        print(f"üîÑ 5 workers, {len(all_times)} operations p√• {total_time:.2f}s")
        if all_times:
            print(f"üìä Genomsnitt: {statistics.mean(all_times):.3f}s, Max: {max(all_times):.3f}s")
    
    def performance_analysis(self):
        """Analysera prestanda resultat"""
        print("\nüìä PRESTANDA ANALYS")
        print("=" * 50)
        
        # Insert performance
        if self.results['insert_times']:
            insert_avg = statistics.mean(self.results['insert_times'])
            insert_max = max(self.results['insert_times'])
            print(f"üî∏ Insert: Genomsnitt {insert_avg*1000:.1f}ms, Max {insert_max*1000:.1f}ms")
        
        # Retrieval performance  
        if self.results['retrieval_times']:
            retrieval_avg = statistics.mean(self.results['retrieval_times'])
            retrieval_max = max(self.results['retrieval_times'])
            print(f"üîç Retrieval: Genomsnitt {retrieval_avg*1000:.1f}ms, Max {retrieval_max*1000:.1f}ms")
        
        # Context performance
        if self.results['context_times']:
            context_avg = statistics.mean(self.results['context_times'])
            context_max = max(self.results['context_times'])  
            print(f"üí¨ Context: Genomsnitt {context_avg*1000:.1f}ms, Max {context_max*1000:.1f}ms")
        
        # Chunking performance
        if self.results['chunking_times']:
            chunk_avg = statistics.mean(self.results['chunking_times'])
            chunk_max = max(self.results['chunking_times'])
            print(f"üìù Chunking: Genomsnitt {chunk_avg*1000:.1f}ms, Max {chunk_max*1000:.1f}ms")
        
        # Errors
        if self.results['errors']:
            print(f"\n‚ùå FEL: {len(self.results['errors'])} fel uppt√§ckta:")
            for error in self.results['errors'][:5]:  # Visa f√∂rsta 5
                print(f"   - {error}")
            if len(self.results['errors']) > 5:
                print(f"   ... och {len(self.results['errors'])-5} till")
        else:
            print("\n‚úÖ Inga fel uppt√§ckta!")
        
        # Database stats
        try:
            with self.memory._conn() as c:
                memories_count = c.execute("SELECT COUNT(*) FROM memories").fetchone()[0]
                conversations_count = c.execute("SELECT COUNT(*) FROM conversations").fetchone()[0]
                print(f"\nüìä Databas: {memories_count} memories, {conversations_count} conversation turns")
        except Exception as e:
            print(f"\n‚ùå Kunde inte h√§mta databas stats: {e}")
    
    def run_all_tests(self):
        """K√∂r alla stresstests"""
        print("üöÄ ALICE RAG/CONTEXT STRESSTEST STARTAR")
        print("=" * 50)
        
        start_time = time.time()
        
        try:
            self.setup_test_data()
            self.test_retrieval_performance()
            self.test_conversation_context()
            self.concurrent_stress_test()
            
            total_time = time.time() - start_time
            print(f"\n‚è±Ô∏è  Total testtid: {total_time:.2f}s")
            
            self.performance_analysis()
            
        finally:
            self.cleanup()
            print(f"\nüßπ Testdatabas rensad")

if __name__ == "__main__":
    test = RAGStressTest()
    test.run_all_tests()